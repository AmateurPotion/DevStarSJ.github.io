<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Coursera Kaggle 강의(How to win a data science competition) week 4-4 Ensemble 요약 | Dev Star SJ</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Coursera Kaggle 강의(How to win a data science competition) week 4-4 Ensemble 요약 Ensemble  여러가지 머신 러닝 모델을 결합하는 방법  1. Averaging (or blending) 여러 모델의 결과값의 평균을 취함  위와 같은 2개의 모델이 있을 경우 1.1 Averaging 단순 평균">
<meta name="keywords" content="MachineLearning,DataScience,Kaggle">
<meta property="og:type" content="article">
<meta property="og:title" content="Coursera Kaggle 강의(How to win a data science competition) week 4-4 Ensemble 요약">
<meta property="og:url" content="http://DevStarSJ.github.io/2018/10/30/kaggle.coursera.competition.04.02/index.html">
<meta property="og:site_name" content="Dev Star SJ">
<meta property="og:description" content="Coursera Kaggle 강의(How to win a data science competition) week 4-4 Ensemble 요약 Ensemble  여러가지 머신 러닝 모델을 결합하는 방법  1. Averaging (or blending) 여러 모델의 결과값의 평균을 취함  위와 같은 2개의 모델이 있을 경우 1.1 Averaging 단순 평균">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.01.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.02.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.02.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.04.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.05.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.06.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.07.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.08.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.09.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.10.png">
<meta property="og:updated_time" content="2018-10-30T08:53:49.680Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Coursera Kaggle 강의(How to win a data science competition) week 4-4 Ensemble 요약">
<meta name="twitter:description" content="Coursera Kaggle 강의(How to win a data science competition) week 4-4 Ensemble 요약 Ensemble  여러가지 머신 러닝 모델을 결합하는 방법  1. Averaging (or blending) 여러 모델의 결과값의 평균을 취함  위와 같은 2개의 모델이 있을 경우 1.1 Averaging 단순 평균">
<meta name="twitter:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.01.png">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-104294646-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics --><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Dev Star SJ</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">个人博客</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
      </nav>
      <nav id="sub-nav">
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://DevStarSJ.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-kaggle.coursera.competition.04.02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/30/kaggle.coursera.competition.04.02/" class="article-date">
  <time datetime="2018-10-30T08:53:00.000Z" itemprop="datePublished">2018-10-30</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/DataScience/">DataScience</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Coursera Kaggle 강의(How to win a data science competition) week 4-4 Ensemble 요약
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1>Coursera Kaggle 강의(How to win a data science competition) week 4-4 Ensemble 요약</h1>
<h2>Ensemble</h2>
<ul>
<li>여러가지 머신 러닝 모델을 결합하는 방법</li>
</ul>
<h3>1. Averaging (or blending)</h3>
<p>여러 모델의 결과값의 평균을 취함</p>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.01.png" alt=""></p>
<p>위와 같은 2개의 모델이 있을 경우</p>
<h4>1.1 Averaging</h4>
<p>단순 평균</p>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.02.png" alt=""></p>
<h4>1.2 Weighted averaging</h4>
<p>가중 평균</p>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.02.png" alt=""></p>
<h4>1.3 Conditional averaging</h4>
<p>조건 평균</p>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.04.png" alt=""></p>
<h3>2. Bagging</h3>
<p>동일한 모델의 약간 다른 버전의 평균. 대표적인 예로 Random Forest</p>
<p>Bagging을 하는 이유로는 모델이 가지는 2가지 문제를 해결하기 위해서다.</p>
<ol>
<li>Underfitting : Bias</li>
<li>Overfitting : Variance</li>
</ol>
<p>Bagging용 파라메터</p>
<ul>
<li>Random seed</li>
<li>Row (Sub) sampling or Bootstrapping</li>
<li>Shuffling</li>
<li>Column (sub) sampling</li>
<li>Model-specific parameters</li>
<li>Number of models (or bags) : 보통 10 이상으로 하지만, 어느 순간 성능의 정체상태가 온다.</li>
</ul>
<p>Bagging 예제 코드 : <code>BaggingClasifier</code> and <code>BeggingRegressor</code> from sklearn</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">model = RandomForestRegressor()</div><div class="line">bags = <span class="number">10</span></div><div class="line">seed = <span class="number">1</span></div><div class="line"></div><div class="line">bagged_prediction = np.zeros(test.shape[<span class="number">0</span>])</div><div class="line"></div><div class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">0</span>, bags):</div><div class="line">    model.set_params(random_state = seed + n)</div><div class="line">    model.fit(train, y)</div><div class="line">    preds = mpdel.predict(test)</div><div class="line">    bagged_prediction += preds</div><div class="line"></div><div class="line">bagged_prediction /= bags</div></pre></td></tr></table></figure></p>
<h3>3. Boosting</h3>
<p>Boosting이란 이전 모델의 성능을 고려한 방식으로 순차적으로 구축되는 모델의 가중 평균으로 구하는 방식이다.</p>
<p>Boosting에는 2가지 방법이 있다.</p>
<ol>
<li>Weight based Boosting (웨이트 기반)</li>
<li>Residual based Boosting (잔여 오류 기반)</li>
</ol>
<h4>3.1 Weight based Boosting</h4>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.05.png" alt=""></p>
<p>위 예에서 우리가 구한 predict값이 <code>pred</code>라고 했을시 타겟(y)과의 절대차를 <code>abs.error</code>라고 했을 경우, 1에다가 절대차를 더한 값으로 <code>weight</code>로 설정한다. weight를 계산하는 방법은 이것 말고도 다른 방법들이 있다.</p>
<p>다음에 학습시킬 때는 이 weight를 feature에 추가하여 진행한다.</p>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.06.png" alt=""></p>
<p>Weight based Boosting의 parameter들</p>
<ul>
<li>Learning rate</li>
<li>Number of estimators</li>
<li>Input model</li>
<li>Sub boosting type : AdaBoost (sklearn), LogitBoost (Weka - Java)</li>
</ul>
<h4>3.2 Residual based boosting</h4>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.07.png" alt=""></p>
<p>이번에는 <code>pred</code>값으로 <code>error</code>값을 구한 다음 그 error를 타겟으로 해서 학습을 진행한다.</p>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.08.png" alt=""></p>
<p>그렇게 학습한 뒤에 새로운 <code>new_pred</code>값에 기존 <code>pred</code> 값을 더하여 최종 <code>predict</code>값으로 설정한다.<br>
위 그림에서 Rownum = 1의 경우 <code>0.2 + 0.75 = 0.95</code>가 최종 predict가 되는 것이며, Rownum = 4의 경우에는 <code>-0.2 + 0.55 = 0.35</code>가 되는 것이다. 이런식으로 계산하면 정확도가 많이 향상된다.</p>
<p>Weight based Boosting의 parameter들</p>
<ul>
<li>Learning rate</li>
<li>Number of estimators</li>
<li>Row (sub) sampling</li>
<li>Column (sub) sampling</li>
<li>Input model : tree 모델이 좋음</li>
<li>Sub boosting type: Fully gradient based, Dart</li>
</ul>
<p>Residual based로 구현된 모델로는 XGBoost, LightGBM, H2O's GBM, CatBoost, Sklearn's GBM 등이 있다.</p>
<h3>4. Stacking</h3>
<p>여러가지 모델로 학습한 결과로 새로운 데이터셋을 만드는 방법이다. 앙상블 방법 중 가장 인기가 많다.</p>
<p>1992년 Wolpert에 의해서 stacking이 소개 되었다. 그것은 다음과 같다.</p>
<ol>
<li>train 데이터를 2개로 나눈다.</li>
<li>그중 하나로 여러 가지 모델을 학습시킨다. (base learner)</li>
<li>2번에서 생성한 모델들로 prediction을 진행한다</li>
<li>3번에서 생성한 prediction으로 다른 모델을 학습한다. (higher level learner)</li>
</ol>
<p>Stacking example
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</div><div class="line"><span class="keyword">from</span> sklearn.lear_model <span class="keyword">import</span> LinearRegression</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</div><div class="line"></div><div class="line">training, valid, ytraining, yvalid = train_test_split(train, y, test_size=<span class="number">0.5</span>)</div><div class="line"></div><div class="line">model1 = RandomForestRegerssor()</div><div class="line">model2 = LinearRegression()</div><div class="line"></div><div class="line">model1.fit(training, ytraining)</div><div class="line">model2.fit(training, ytraining)</div><div class="line"></div><div class="line">preds1 = model1.predict(valid)</div><div class="line">preds2 = model2.predcit(valid)</div><div class="line"></div><div class="line">test_preds1 = model1.predict(test)</div><div class="line">test_preds2 = model2.predcit(test)</div><div class="line"></div><div class="line">stacked_predictions = np.column_stack((preds1, preds2))</div><div class="line">stacked_test_predictions = np.column_stack((test_preds1, test_preds2))</div><div class="line"></div><div class="line">meta_model = LinearRegression()</div><div class="line">meta_model.fit(stacked_predictions, yvalid)</div><div class="line"></div><div class="line">final_predictions = meta_model.predict(stacked_test_predictions)</div></pre></td></tr></table></figure></p>
<h3>4. StackNet</h3>
<p>Stacking을 Neural Network 방식으로 여러 Layer로 구성한 방법이다.</p>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.09.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.04.10.png" alt=""></p>
<p>앞서 본 Stacking에서는 train을 2부분으로 나누어서 진행하였다. StackNet에서 Layer를 추가해야한다고 한다면 이걸 다시 나누어야 한다.<br>
우리가 충분히 많은 데이터셋을 가지고 있지 않다면 끊임없이 나누지 않고 작업을 수행해도 된다.</p>
<p>데이터를 나눌때 <code>K-Fold</code> 패러다임을 사용하는 방법이 있다. K = 4 라는 상황을 가정한다면 train 데이터를 4개로 나누어 4개의 모델에 대해서 그 중 3개로 학습하고 1개로 validation한 모델들을 만든 다음 전체 train 데이터에 predict한 결과를 생성하여 Stacking을 수행한다. 또는 평균을 취할 수도 있다.</p>
<p>StakNet에서는 하지만 Neural Network의 back propagation이 지원되지는 않는다. 왜냐면 모든 모델이 다 미분이 가능하지는 않을 수 있기 때문이다.</p>
<h4>1st level tips</h4>
<ul>
<li>
<p>Diversity based on algorithms:</p>
<ul>
<li>2-3 Gradient boosted trees (LightGBM, XGBoost, H2O, CatBoost)</li>
<li>2-3 Neural Nets (Keras, PyTouch)</li>
<li>1-2 ExtraTrees / Random Forest (sklearn)</li>
<li>1-2 Linear Models as in logistic/ridge regression, linear svm (sklearn)</li>
<li>1-2 knn models (sklearn)</li>
<li>1 Factorixation machine (libfm)</li>
<li>1 Svm with nonlinear kernel if size/memory allows (sklearn)</li>
</ul>
</li>
<li>
<p>Diversity based on input data:</p>
<ul>
<li>Categorical features: One hot, Label encoding, Target encodingm Frequency</li>
<li>Numerical features: Outliers, Binning, Derivatives, Percentiles, Scaling</li>
<li>Interactions: col1*/+-col2, groupby, unsupervised</li>
</ul>
</li>
</ul>
<h4>Subsequenct level tips</h4>
<ul>
<li>
<p>Simpler (or shallower) Algorithms:</p>
<ul>
<li>Gradient boosted trees with small depth (like 2 or 3)</li>
<li>Linear models with high regularization</li>
<li>Extra Trees</li>
<li>Shallow networks (as in 1 hidden layer)</li>
<li>Knn with BrayCutis distance</li>
<li>Brute forcing a search for best linear weights based on cv</li>
</ul>
</li>
<li>
<p>Feature engineering</p>
<ul>
<li>pairwise differences between meta features</li>
<li>row-wise statistics like averages or stds</li>
<li>Standard feature selection techniques</li>
</ul>
</li>
<li>
<p>For Every 7.5 models in previous level we add 1 in meta (empirical)</p>
</li>
<li>
<p>Be mindful of target leakage</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://DevStarSJ.github.io/2018/10/30/kaggle.coursera.competition.04.02/" data-id="cjov28bra009m5b7bnf7vtdkc" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DataScience/">DataScience</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kaggle/">Kaggle</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MachineLearning/">MachineLearning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/11/04/kaggle.coursera.competition.03.02/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Coursera Kaggle 강의(How to win a data science competition) week 3,4 Advanced Feature Engineering 요약
        
      </div>
    </a>
  
  
    <a href="/2018/10/30/kaggle.coursera.competition.04.01/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Coursera Kaggle 강의(How to win a data science competition) week 4-1 Hyperparameter Tuning 요약</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    <!-- Featured Tags -->

  
    <!-- Short About -->
<section class="visible-md visible-lg">
    <h5><a href="/about/">ABOUT ME</a></h5>
    <div class="short-about">

        

        

        <!-- SNS Link -->
        <ul class="list-inline">
            
            
            

            

            

            
            
            
            
        </ul>
    </div>
</section>

  
    
  <h5>RECENT POSTS</h3>
  <div class="widget">
    <ul>
      
        <li>
          <a href="/2018/11/24/aws-batch-tutorial/">Introduce to AWS Batch</a>
        </li>
      
        <li>
          <a href="/2018/11/04/kaggle.coursera.competition.03.02/">Coursera Kaggle 강의(How to win a data science competition) week 3,4 Advanced Feature Engineering 요약</a>
        </li>
      
        <li>
          <a href="/2018/10/30/kaggle.coursera.competition.04.02/">Coursera Kaggle 강의(How to win a data science competition) week 4-4 Ensemble 요약</a>
        </li>
      
        <li>
          <a href="/2018/10/30/kaggle.coursera.competition.04.01/">Coursera Kaggle 강의(How to win a data science competition) week 4-1 Hyperparameter Tuning 요약</a>
        </li>
      
        <li>
          <a href="/2018/10/30/181030.data.philosophy/">데이터를 철학하다</a>
        </li>
      
    </ul>
  </div>

  
    <!-- Friends Blog -->

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a><span class="archive-list-count">17</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/06/">June 2013</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Yun Seok-joon<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>



<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>