<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Coursera Kaggle 강의(How to win a data science competition) week 3-1 Metrics 요약 | Dev Star SJ</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Coursera Kaggle 강의(How to win a data science competition) week 3-1 Metrics 요약 Metrics 어떤 metric을 사용하느냐에 따라 모델이 학습하는 방향이 다르다. 우리가 풀고자하는 문제에 최적화된 metric을 선택하는 것이 중요하다. 1. Regression metrics 1.1 MSE, RMS">
<meta name="keywords" content="MachineLearning,DataScience,Kaggle">
<meta property="og:type" content="article">
<meta property="og:title" content="Coursera Kaggle 강의(How to win a data science competition) week 3-1 Metrics 요약">
<meta property="og:url" content="http://DevStarSJ.github.io/2018/10/22/kaggle.coursera.competition.03.01/index.html">
<meta property="og:site_name" content="Dev Star SJ">
<meta property="og:description" content="Coursera Kaggle 강의(How to win a data science competition) week 3-1 Metrics 요약 Metrics 어떤 metric을 사용하느냐에 따라 모델이 학습하는 방향이 다르다. 우리가 풀고자하는 문제에 최적화된 metric을 선택하는 것이 중요하다. 1. Regression metrics 1.1 MSE, RMS">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.01.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.02.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.03.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.04.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.05.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.06.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.07.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.08.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.09.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.10.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.11.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.12.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.13.png">
<meta property="og:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.14.png">
<meta property="og:updated_time" content="2018-11-04T05:17:55.491Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Coursera Kaggle 강의(How to win a data science competition) week 3-1 Metrics 요약">
<meta name="twitter:description" content="Coursera Kaggle 강의(How to win a data science competition) week 3-1 Metrics 요약 Metrics 어떤 metric을 사용하느냐에 따라 모델이 학습하는 방향이 다르다. 우리가 풀고자하는 문제에 최적화된 metric을 선택하는 것이 중요하다. 1. Regression metrics 1.1 MSE, RMS">
<meta name="twitter:image" content="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.01.png">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-104294646-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics --><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Dev Star SJ</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">个人博客</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
      </nav>
      <nav id="sub-nav">
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://DevStarSJ.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-kaggle.coursera.competition.03.01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/22/kaggle.coursera.competition.03.01/" class="article-date">
  <time datetime="2018-10-22T02:04:00.000Z" itemprop="datePublished">2018-10-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/DataScience/">DataScience</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Coursera Kaggle 강의(How to win a data science competition) week 3-1 Metrics 요약
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1>Coursera Kaggle 강의(How to win a data science competition) week 3-1 Metrics 요약</h1>
<h2>Metrics</h2>
<p>어떤 metric을 사용하느냐에 따라 모델이 학습하는 방향이 다르다. 우리가 풀고자하는 문제에 최적화된 metric을 선택하는 것이 중요하다.</p>
<h3>1. Regression metrics</h3>
<h4>1.1 MSE, RMSE, R-squared</h4>
<ul>
<li><strong>MSE</strong> (Mean Square Error) : target과 predict의 차이값의 제곱의 평균
<ul>
<li><code>Best Constant</code> : target mean</li>
<li><code>sklearn.metrics.mean_squared_error</code></li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.01.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.02.png" alt=""></p>
<ul>
<li><strong>RMSE</strong> (Root Mean Square Error): MSE에 root취한 값</li>
</ul>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.03.png" alt=""></p>
<p>MSE의 최소값이 RMSE에서도 최소값이므로 최적화 결과가 같다. 그래서 비교적 구현이 간단한 MSE를 사용하는 경우가 많지만, <strong>learning rate</strong> 같은 몇몇의 hyperparameter 값에 따라 다르게 동작 할 수 있다.<br>
MSE와 RMSE 값이 32라고 성능이 좋은지 나쁜지 판단이 힘들다. 그래서 상대적인 값으로 평가가 필요할 수 있다.</p>
<ul>
<li><strong>R2</strong> (R Squared)
<ul>
<li><code>sklearn.metrics.r2_score</code></li>
<li>P-Value와 같이 0 ~ 1 사이의 값을 나타내는데, MSE가 0 이면 R2는 1이며, MSE가 constant 모델보다 클 때 R2는 0이다.</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.04.png" alt=""></p>
<h4>1.2 MAE, RMAE</h4>
<ul>
<li><strong>MAE</strong> (Mean Absolute Error): target과 predict의 차이 절대값
<ul>
<li><code>Best Constant</code> : target median</li>
<li><code>sklearn.metrics.mean_absolute_error</code></li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.05.png" alt=""></p>
<ul>
<li><strong>RMAE</strong> (Root Mean Absolute Error): MAE에 root취한 값</li>
</ul>
<h4>1.3 MSE와 MAE의 차이</h4>
<ul>
<li>MSE의 경우 차이가 2배이면 error가 4배가 되는데, MAE는 차이가 2배이면 error도 2배</li>
<li>MSE와 RMSE는 최적해를 찾기위해 gradient하게 접근시 각 지점마다 기울기(미분값)이 다르지만, MAE는 왼쪽은 -1, 오른쪽은 1이다.</li>
<li>MAE는 outlier에 덜 민감하게 동작한다. (MSE는 제곱을 해서 크게 민감하다.)</li>
<li>outlier가 없다고 확신이 드는 경우에는 MSE가 더 좋은 경우가 많다.</li>
</ul>
<h4>1.4 (R)MSPE (Mean Square Percent Error), (R)MAPE(Mean Absolute Percent Error) : relative_metric</h4>
<p>MSE 와 MAE는 error를 절대적인 값으로 비교를 한다. 그래서 <code>9 -&gt; 10 (MAE, MSE=1)</code> 와 <code>999 -&gt; 1000 (MAE, MSE=1)</code>는 같은 양의 error로 계산된다.
하지만 <code>900 -&gt; 1000 (MAE=100, MSE=10000)</code>는 error의 수치가 훨씬 커진다.</p>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.06.png" alt=""></p>
<p>MSPE와 MAPE는 각각 MSE와 MAE에다가 전체 데이터 개수의 퍼센트로 계산한다.</p>
<p><code>Best Constant</code> 역시 MSPE는 <em>weighted target mean</em>이며, MAPE는 <em>weighted target median</em> 값이다.<br>
앞에 Root가 붙은 버전에 대한 설명은 생략한다.</p>
<h4>1.5 RMSLE (Root Mean Square Logarithmic Error)</h4>
<ul>
<li><code>sklearn.metrics.mean_squared_log_error</code></li>
</ul>
<p>로그 스케일로 계산된 RMSE</p>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.07.png" alt=""></p>
<p>Error 곡선의 좌우가 대칭적이지 않다.</p>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.08.png" alt=""></p>
<h3>2. Classification metrics</h3>
<h4>2.1 Accuracy</h4>
<ul>
<li><code>sklearn.metrics.accuracy_score</code></li>
</ul>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.09.png" alt=""></p>
<p>예측한 값과 target값이 같으면 1 아니면 0 으로 계산하여 평균을 취한 값<br>
개 그림 맞추기 문제에서 <em>개</em> 90, <em>고양이</em> 10으로 데이터셋이 있는 경우 모두 <em>개</em>라고 대답해도 Accuracy는 0.9가 나온다.</p>
<h4>2.2 Logarithmic loss (logloss)</h4>
<ul>
<li><code>sklearn.metrics.log_loss</code></li>
</ul>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.10.png" alt=""></p>
<p>target과 차이가 클수록 penalty가 커진다. 하나의 큰 error는 여러 개의 작은 error들보다 훨씬 더 penalty가 크다.</p>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.11.png" alt=""></p>
<h4>2.3 AUC (Area Under Curve)</h4>
<p>얼마나 구분을 잘하느냐, 얼마나 겹치는게 없느냐에 대한 검증
좋은 피처인지 아닌지를 구분할때 많이 사용</p>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.12.png" alt=""></p>
<p><code>True Positive</code> 와  <code>False Positive</code>를 이용하여 <code>TP</code>는 위쪽 <code>FP</code>는 오른쪽으로 움직이여 곡선을 그림을 그려서 그 아래 면적으로 평가. 면적이 넓을 수록 좋음. 최고 점수는 1</p>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.13.png" alt=""></p>
<h4>2.4 Cohen's Kappa</h4>
<p>어렵다. 무슨 내용인지 도저히 모르겠음</p>
<h3>3. General approaches for metrics optimization</h3>
<ul>
<li>Target metric : 우리가 최적화 하려는 것</li>
<li>Optimization loss : 모델이 최적화 하는 것</li>
</ul>
<p>모델은 우리가 지정한 <em>metric</em>으로 계산한 <em>loss</em>를 최소화하기 위해서 학습한다.</p>
<ul>
<li>metric 최적화 전략
<ul>
<li>처음에 그냥 model을 실행할때 : MSE, Logloss</li>
<li>train 데이터 전처리하여 다른 metric에 최적화 할때 : MSPE, MAPE, RMSLE, ...</li>
<li>다른 metric이나 predict 결과를 후처리 할때 : Accuracy, Kappa</li>
<li>기타 : loss function를 스스로 작성</li>
<li>다른 metric의 early stopping 용 : Any...</li>
</ul>
</li>
</ul>
<p>ex) 1,2차 미분값으로 loss를 계산하고자 할 때</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">logregobj</span><span class="params">(preds, dtrain)</span>:</span></div><div class="line">    labels = dtrain.get_label()</div><div class="line">    preds = <span class="number">1.</span> / (<span class="number">1.</span> + np.exp(-preds))</div><div class="line">    grad = preds - labels </div><div class="line">    hess = preds * (<span class="number">1.</span> - preds)</div><div class="line">    <span class="keyword">return</span> grad, hess</div></pre></td></tr></table></figure></p>
<ul>
<li>Early stopping
<ul>
<li>M1 metric을 최적화하기 위해서 M2 metric의 최적값을 구하는 경우</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/DevStarSJ/Study/master/Blog/Kaggle/Coursera.competition/image/coursera.competition.03.14.png" alt=""></p>
<h3>4. Regression metrics optimization</h3>
<h4>4.1 MSE, RMSE, R-squared</h4>
<p>대부분의 모델에서 잘 동작한다.</p>
<ul>
<li>Tree-based : XGBoost, LightGBM, sklearn.RandomForestRegressor</li>
<li>Linear models : sklearn.&lt;&gt;Regression, sklearn.SGFRegressor, Vowpal Wabbit (quantile loss)</li>
<li>Neural nets : Pytorchm Keras, TF, etc.</li>
</ul>
<p>L2 loss라고도 불림</p>
<h4>4.2 MAE</h4>
<p>MSE와 최적화 관점에서는 큰 차이가 없지만, 2차 미분이 0이기 때문에 extra boost 방식에서는 사용을 못한다.</p>
<ul>
<li>Tree-based : LightGBM, sklearn.RandomForestRegressor</li>
<li>Linear models : Vowpal Wabbit (quantile loss)</li>
<li>Neural nets : Pytorchm Keras, TF, etc.</li>
</ul>
<p>L1 loss, Median regression 이라고도 불림</p>
<h4>4.3 MSPE, MAPE</h4>
<p>이것은 앞에서 본 2가지 metric과는 용도가 다르다.<br>
각각 MSE, MAE의 가중 버전이다.<br>
다른 metric의 early stopping을 위해 사용되기도 한다.</p>
<ul>
<li>XGBoost, LightGBM 에서만 <code>sample_weights</code>용으로 사용한다.</li>
</ul>
<h4>4.4 RMSLE</h4>
<p>로그공간에서의 MSE 최적화 방식</p>
<h3>5. Classification metrics optimization</h3>
<h4>5.1 LogLoss (Logistic Loss)</h4>
<ul>
<li>모델이 잘 맞추는지를 측정</li>
<li>Regression의 MAE와 같은 존재</li>
<li>RandomForest빼고는 대부분 잘 맞음
<ul>
<li>Tree-based : XGBoost, LightGBM</li>
<li>Linear models : sklearn.&lt;&gt;Regression, sklearn.SGDRegressor, Vowpal Wabbit</li>
<li>Neural nets : Pytorchm Keras, TF, etc.</li>
</ul>
</li>
</ul>
<h4>5.2 Accuracy</h4>
<ul>
<li>metric이나 treshold를 최적화하는데 유용함</li>
</ul>
<h4>5.3 AUC</h4>
<ul>
<li>모델이 잘 맞추는것을 측정하거나 logloss를 최적화하기 위해 사용됨</li>
<li>한상의 object가 올바른 순서로 정렬될 확률</li>
<li>Tree-based : XGBoost, LightGBM</li>
<li>Neural nets : Pytorchm Keras, TF, etc.</li>
</ul>
<h4>5.4</h4>
<ul>
<li>MSE를 최적화</li>
<li>올바른 threshold를 찾음
<ul>
<li>Bad : np.round(predictions)</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://DevStarSJ.github.io/2018/10/22/kaggle.coursera.competition.03.01/" data-id="cjov28br4009a5b7b1swjll87" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DataScience/">DataScience</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kaggle/">Kaggle</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MachineLearning/">MachineLearning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/10/30/181030.data.philosophy/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          데이터를 철학하다
        
      </div>
    </a>
  
  
    <a href="/2018/10/21/kaggle.coursera.competition.02.02/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Coursera Kaggle 강의(How to win a data science competition) week 2-2 Validation 요약</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    <!-- Featured Tags -->

  
    <!-- Short About -->
<section class="visible-md visible-lg">
    <h5><a href="/about/">ABOUT ME</a></h5>
    <div class="short-about">

        

        

        <!-- SNS Link -->
        <ul class="list-inline">
            
            
            

            

            

            
            
            
            
        </ul>
    </div>
</section>

  
    
  <h5>RECENT POSTS</h3>
  <div class="widget">
    <ul>
      
        <li>
          <a href="/2018/11/24/aws-batch-tutorial/">Introduce to AWS Batch</a>
        </li>
      
        <li>
          <a href="/2018/11/04/kaggle.coursera.competition.03.02/">Coursera Kaggle 강의(How to win a data science competition) week 3,4 Advanced Feature Engineering 요약</a>
        </li>
      
        <li>
          <a href="/2018/10/30/kaggle.coursera.competition.04.02/">Coursera Kaggle 강의(How to win a data science competition) week 4-4 Ensemble 요약</a>
        </li>
      
        <li>
          <a href="/2018/10/30/kaggle.coursera.competition.04.01/">Coursera Kaggle 강의(How to win a data science competition) week 4-1 Hyperparameter Tuning 요약</a>
        </li>
      
        <li>
          <a href="/2018/10/30/181030.data.philosophy/">데이터를 철학하다</a>
        </li>
      
    </ul>
  </div>

  
    <!-- Friends Blog -->

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a><span class="archive-list-count">17</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/06/">June 2013</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Yun Seok-joon<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>



<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>